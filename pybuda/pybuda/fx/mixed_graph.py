# SPDX-FileCopyrightText: Â© 2024 Tenstorrent AI ULC

# SPDX-License-Identifier: Apache-2.0

#
# Mixed graph contains a pybuda graph, and one or more FX graphs that will be executed on the CPU. It is 
# generated by capturing a FX graph from pt2. Unsupported ops, or arguments will be dropped down to CPU.
#

from typing import Dict, List, Tuple, Set

import torch
from loguru import logger

import pybuda
from pybuda.fx.nodes import torch_constant_ops, call_function_is_nop
from pybuda.fx.schedule import Schedule
from pybuda.fx.graph_utils import flatten_args, reduce_graph, get_output_node, append_to_output, move_output_to_end, remove_output_index, graph_lint
from pybuda._C.torch_device import unique_id

class MixedGraph:
    def __init__(self, module_name: str):
        self.graph = pybuda._C.graph.Graph(module_name)
        self.inputs_per_subgraph : Dict[int, List[int]] = {}
        self.outputs_per_subgraph : Dict[int, List[int]] = {}

        # Original graph nodes, ordered before the filtering
        self.input_nodes_per_subgraph : Dict[int, List[torch.fx.Node]] = {}
        self.output_nodes_per_subgraph : Dict[int, List[torch.fx.Node]] = {}

        self.fallback_graphs_per_subgraph: Dict[int, List[torch.fx.Graph]] = {}
        self.mappings_per_subgraph: Dict[int, Dict[str, Dict[torch.fx.Node, torch.fx.Node]]] = {}
        self.aten_graph_per_subgraph: Dict[int, torch.fx.Graph] = {}

    def capture_sample_inputs(self, inputs: List[torch.Tensor], subgraph_id: int):
        self.inputs_per_subgraph[subgraph_id] = [unique_id(t) for t in inputs]

    def capture_sample_outputs(self, outputs: List[torch.Tensor], subgraph_id: int):
        self.outputs_per_subgraph[subgraph_id] = [unique_id(t) for t in outputs]

    def get_subgraph_input(self, subgraph_id: int, input_index: int) -> int:
        return self.inputs_per_subgraph[subgraph_id][input_index]

    def get_output_index(self, uid) -> Tuple[int, int]:
        # Return the subgraph index and output index for the given uid
        for idx in self.outputs_per_subgraph:
            if uid in self.outputs_per_subgraph[idx]:
                return idx, self.outputs_per_subgraph[idx].index(uid)

        assert False, "Output not found"

    def filter_unsupported_nodes(self, aten_module: torch.nn.Module, unsupported_ops: Set[torch.fx.Node], subgraph_id: int):
        # Move unsupported ops to CPU

        # First, we'll copy all unsupported ops to a new FX graph. For each node that gets its input
        # from a supported op, we'll create a new input in the fallback graph, and output in the original graph.
        # We'll then record that mapping.

        # Mapping of input node on one graph, and node that drives the output on the other graph that should feed this input
        new_io_mapping : Dict[torch.fx.Node, torch.fx.Node] = {}

        # Placeholders that are copied into the new graph
        placeholder_map : Dict[torch.fx.Node, torch.fx.Node] = {}

        # Mapping of copied nodes
        copied_node_mapping : Dict[torch.fx.Node, torch.fx.Node] = {}

        # Mapping of outputs that got moved out of main graph into fallback, to fallback's output
        moved_output_mapping : Dict[torch.fx.Node, torch.fx.Node] = {}

        # List of inputs/outputs to create in the original graph, once we're done traversing and it's safe to modify
        scheduled_new_outputs : List[Tuple[torch.fx.Node, torch.fx.Node]] = [] # Tuple - source in original graph, dest in new graph
        scheduled_new_inputs : List[Tuple[torch.fx.Node, torch.fx.Node, torch.fx.Node]] = [] # Tuple - dest in original graph, source in original graph, source in new graph

        fallback_graph = torch.fx.Graph()

        if subgraph_id not in self.input_nodes_per_subgraph:
            self.input_nodes_per_subgraph[subgraph_id] = []

        if subgraph_id not in self.output_nodes_per_subgraph:
            self.output_nodes_per_subgraph[subgraph_id] = []

        # Some of the unsupported nodes will have nop arguments, which we want to copy over as well. Let's find those first.
        to_copy_ops = unsupported_ops.copy()
        for node in aten_module.graph.nodes:
                
            # While traversing, record original inputs/outputs
            if node.op == "placeholder":
                self.input_nodes_per_subgraph[subgraph_id].append(node)
                continue

            if node.op == "output":
                for arg in node.args[0]:
                    self.output_nodes_per_subgraph[subgraph_id].append(arg)
                continue

            if node not in unsupported_ops:
                continue
            
            # Collect the inputs
            node_args = flatten_args(node.args)

            for arg in node_args:
                if arg.op != "call_function":
                    continue
                
                op_name = arg.target.__name__
                if call_function_is_nop(arg):
                    to_copy_ops.add(arg)

        # Now go through and copy the nodes that need copying
        for node in aten_module.graph.nodes:

            # If the output is driven by an unsupported op, then we need to move it over to the new graph
            if node.op == "output":
                assert len(node.args) == 1
                for driving_node in node.args[0]:
                    if driving_node in unsupported_ops:
                        logger.trace(f"Moving output: {driving_node} {hex(id(driving_node))}, to copied node: {hex(id(copied_node_mapping[driving_node]))}")
                        
                        # Add to fallback graph
                        append_to_output(fallback_graph, copied_node_mapping[driving_node])
                        moved_output_mapping[driving_node] = copied_node_mapping[driving_node]
                        
                continue

            # If the op is supported, but has an argument that's unsupported, then we need to create an output on fallback graph to get
            # the value, and a placeholder in the original graph, and map them 
            if node not in to_copy_ops:
                if not isinstance(node, torch.fx.Node):
                    continue
                args = flatten_args(node.args)
                for arg in args:
                    if arg not in unsupported_ops:
                        continue
                    
                    # Check if we already have this input scheduled
                    already_scheduled = False
                    for _, n, _ in scheduled_new_inputs:
                        if n == arg:
                            already_scheduled = True
                            break

                    if already_scheduled:
                        continue
                    
                    # We should've already made a copy of this, so it's safe to get it from copied_node_mapping
                    new_node = copied_node_mapping[arg]
                    append_to_output(fallback_graph, new_node)
                    scheduled_new_inputs.append((node, arg, new_node))

                continue

            if node not in to_copy_ops:
                continue

            # Collect the inputs
            node_args = flatten_args(node.args)

            # Figure out which of these need to be new inputs, and also create the map for copying
            arg_map : Dict[torch.fx.Node, torch.fx.Node] = {}

            def device_kwarg_to_cpu(node: torch.fx.Node):
                # If the node is a device kwarg, then we need to move it to CPU
                if 'device' in node.kwargs:
                    new_kwargs = node.kwargs.copy()
                    new_kwargs['device'] = 'cpu'
                    node.kwargs = new_kwargs

            for arg in node_args:
                if arg in to_copy_ops:
                    # We should've already made a copy of this
                    assert arg in copied_node_mapping, f"Node {arg} not copied, but it's an argument to {node} and in 'to copy ops'"
                    arg_map[arg] = copied_node_mapping[arg]
                    continue
            
                if arg.op == "call_function":
                    op_name = arg.target.__name__

                    # If the function is a constant, copy it over, no sense it evaluating it on device and then copying
                    if op_name in torch_constant_ops or op_name == "getitem":
                        logger.trace(f"Copying constant op to fallback graph: {arg}")
                        new_node = fallback_graph.node_copy(arg, lambda x: x) # there should be no node args to copy in a constant op
                        copied_node_mapping[arg] = new_node
                        arg_map[arg] = new_node
                        device_kwarg_to_cpu(copied_node_mapping[arg])
                        continue

                    # Supported op, calculated on device. We need to create inputs and outputs.
                    logger.trace(f"Creating new output/input pair to fallback graph: {arg}")
                    in_node = fallback_graph.placeholder(arg.name)
                    scheduled_new_outputs.append((arg, in_node))
                    arg_map[arg] = in_node
                    continue

                if arg.op == "placeholder":
                    # We need to create a new placeholder in the fallback graph
                    logger.trace(f"Copying placeholder to fallback graph: {arg}")
                    in_node = fallback_graph.placeholder(arg.name)
                    arg_map[arg] = in_node
                    placeholder_map[in_node] = arg
                    continue

                # Explicitly allow other types for now, so that we don't miss anything important. Eventually we can remove the assert
                if arg.op in ["int", "float"]:
                    continue

                assert False, f"Unsupported argument type {arg.op}"
                

            logger.trace(f"Falling back unsupported op, or needed nop, to fallback graph: {node}")
            copied_node_mapping[node] = fallback_graph.node_copy(node, lambda x: arg_map[x])
            device_kwarg_to_cpu(copied_node_mapping[node])

        # Create new outputs
        # Graph can only have one output, so we need to append it to existing output
        for source, dest in scheduled_new_outputs:
            append_to_output(aten_module.graph, source)
            new_io_mapping[dest] = source

        # Create new inputs
        for dest, source, new_source in scheduled_new_inputs:
            with aten_module.graph.inserting_before(dest):
                in_node = aten_module.graph.placeholder(source.name)
                in_node.meta["tensor_meta"] = source.meta["tensor_meta"]
            source.replace_all_uses_with(in_node)
            new_io_mapping[in_node] = new_source
        
        # Remove outputs
        output_node = get_output_node(aten_module.graph)
        assert output_node is not None
        for driving_node in moved_output_mapping:
            remove_output_index(output_node, output_node.args[0].index(driving_node))

        # Remove the unsupported ops from the original graph
        for node in reversed(aten_module.graph.nodes):
            if node in unsupported_ops:
                aten_module.graph.erase_node(node)
        
        # Reduce unused stuff
        reduce_graph(aten_module)

        # Move outputs to the end
        move_output_to_end(aten_module.graph)
        move_output_to_end(fallback_graph)

        graph_lint(aten_module.graph, "Device_graph_after_fallback")
        graph_lint(fallback_graph, "Merged_fallback")

        #print("After fallback:")
        #print("Original graph:")
        #aten_module.graph.print_tabular()
        #print("Fallback graph:")
        #fallback_graph.print_tabular()
        #print("IO Mappings: ", new_io_mapping)

        # Break up fallback graph into multiple graphs to break any deadlocks. Ouptuts to intermediates and
        # inputs from intermediates can't be in the same graph.
        progress = len(fallback_graph.nodes) > 0 # skip this if the graph is already empty
        fallback_graphs = [fallback_graph]
        new_graphs = []
        while progress:
            # Keep looking for input/output pair, make a copy of the graph and them remove down from input and up from output
            # If either/both graphs end up empty, then they are linked and we have a problem

            # There are probably better algorithms to do this :) - but this is simple and I think it works
            fallback_graphs.extend(new_graphs)
            new_graphs = []
            intermediate_outputs = set(new_io_mapping.values())
            progress = False

            for graph in fallback_graphs:
                # Find an input from intermediates
                intermediate_input = None
                for node in graph.nodes:
                    if node in new_io_mapping:
                        intermediate_input = node
                        break

                # Find an output to intermediate
                intermediate_output = None
                if intermediate_input is not None:
                    for node in graph.nodes:
                        if node in intermediate_outputs:
                            intermediate_output = node
                            break

                if intermediate_input is None or intermediate_output is None:
                    continue

                # Create a copy, and separate graphs
                logger.debug(f"Fallback graph has a dependency to break: {intermediate_output} -> {intermediate_input}")
                new_graph = torch.fx.Graph()
                #new_graph = copy.deepcopy(graph)
                copy_map = {}
                output_value = new_graph.graph_copy(graph, copy_map)
                new_output = new_graph.output(output_value)
                new_output.meta["tensor_meta"] = tuple(o.meta["tensor_meta"] for o in output_value)

                # Remove nodes from the original graph
                to_remove = []
                to_process = [intermediate_input]
                while to_process:
                    node = to_process.pop()
                    to_remove.append(node)
                    for user in node.users:
                        if user not in to_remove:
                            if user.op == "output":
                                remove_output_index(user, user.args[0].index(node))
                                break # done at output
                            else:
                                to_process.append(user)

                for node in reversed(to_remove):
                    graph.erase_node(node)

                reduce_graph(graph) # remove dead code
                assert len(graph.nodes) > 0, f"Graph {graph} is empty after trying to disjoin multiple fallback graphs"

                # Removed nodes from the copy, starting from the output working our way up
                to_remove = []
                graph_output = get_output_node(new_graph)
                remove_output_index(graph_output, graph_output.args[0].index(copy_map[intermediate_output]))
                to_process = [copy_map[intermediate_output]]
                while to_process:
                    node = to_process.pop()
                    to_remove.append(node)
                    if not isinstance(node, torch.fx.Node):
                        continue

                    args = flatten_args(node.args)
                    for arg in args:
                        if not isinstance(arg, torch.fx.Node):
                            continue
                        if arg not in to_remove:
                            to_process.append(arg)
                
                for node in to_remove:
                    new_graph.erase_node(node)

                reduce_graph(new_graph)

                assert len(new_graph.nodes) > 0, f"Graph {new_graph} is empty after trying to disjoin multiple fallback graphs"

                new_graphs.append(new_graph)
                progress = True

                # Update mappings, as some nodes have moved into the new graph
                for node in new_graph.nodes:
                    if node.op == "output":
                        continue

                    for node_map in [new_io_mapping, placeholder_map, copied_node_mapping, moved_output_mapping]:
                        # Reverse lookup, since copy map is old->new, and we need new->old
                        original_node = None
                        for org_node, new_node in copy_map.items():
                            if new_node == node:
                                original_node = org_node
                                break
                        assert original_node is not None
                        update_node_map(node_map, original_node, node)


        # Update output nodes
        for i, node in enumerate(self.output_nodes_per_subgraph[subgraph_id]):
            if node in moved_output_mapping:
                self.output_nodes_per_subgraph[subgraph_id][i] = moved_output_mapping[node]

        
        graph_lint(aten_module.graph, "Device_graph_after_fallback_separation")
        [graph_lint(f, f"Fallback_{i}") for i, f in enumerate(fallback_graphs)]

        #print("Final graphs:")
        #print("Device:")
        #aten_module.graph.print_tabular()
        #print("Fallbacks:")
        #for f in fallback_graphs:
        #    f.print_tabular()

        self.fallback_graphs_per_subgraph[subgraph_id] = fallback_graphs if len(fallback_graph.nodes) > 0 else []
        self.mappings_per_subgraph[subgraph_id] = {
                "new_io_mapping": new_io_mapping,
                "placeholder_map": placeholder_map,
                "copied_node_mapping": copied_node_mapping,
                "moved_output_mapping": moved_output_mapping
        }
        self.aten_graph_per_subgraph[subgraph_id] = aten_module.graph

    def generate_schedule(self, subgraph_idx: int) -> Schedule:
        # For given subgraph, figure out a schedule of FX and Buda graphs that need to run, and how to map inputs to outputs
        schedule = Schedule(
                subgraph_idx, 
                self.input_nodes_per_subgraph[subgraph_idx], 
                self.output_nodes_per_subgraph[subgraph_idx], 
                self.aten_graph_per_subgraph[subgraph_idx],
                self.fallback_graphs_per_subgraph[subgraph_idx], 
                self.mappings_per_subgraph[subgraph_idx])


        return schedule

def update_node_map(node_map: Dict[torch.fx.Node, torch.fx.Node], org_node: torch.fx.Node, new_node: torch.fx.Node):
    # Check both keys and values of the map for org_node and switch them to new_node
    if org_node in node_map:
        node_map[new_node] = node_map[org_node]
        del node_map[org_node]

    for k, v in node_map.items():
        if v == org_node:
            node_map[k] = new_node
            break

